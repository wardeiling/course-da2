---
title: "Practical 4: Hierarchical and k-means clustering"
author: "Ward B. Eiling"
format: 
  html:
    self-contained: true
execute:
  warning: false
  message: false
---

# 1 Introduction

In this practical, we will apply hierarchical and k-means clustering to two synthetic datasets. We use the following packages:

```{r}
library(MASS) 
library(tidyverse) 
library(patchwork) 
library(ggdendro) 
```

The data can be generated by running the code below.

# 2 Take-home exercises

**1. The code does not have comments. Add descriptive comments to the code below.**

```{r}
# set the seed for reproducibility
set.seed(123)

# Create the variance-covariance matrix
sigma      <- matrix(c(1, .5, .5, 1), 2, 2)

# simulate two variables with mean 5 and var-cov matrix Sigma
sim_matrix <- mvrnorm(n = 100, mu = c(5, 5), 
                      Sigma = sigma)
colnames(sim_matrix) <- c("x1", "x2")

# convert to tibble
sim_df <- 
  sim_matrix %>% 
  as_tibble() %>%
  # create class variable with equal chance to be assigned to A, B, or C
  mutate(class = sample(c("A", "B", "C"), size = 100, 
                        replace = TRUE))

### for each class systematically adjust values of x1 and x2

# with small effect size
sim_df_small <- 
  sim_df %>%
  mutate(x2 = case_when(class == "A" ~ x2 + .5,
                        class == "B" ~ x2 - .5,
                        class == "C" ~ x2 + .5),
         x1 = case_when(class == "A" ~ x1 - .5,
                        class == "B" ~ x1 - 0,
                        class == "C" ~ x1 + .5))

# with large effect size
sim_df_large <- 
  sim_df %>%
  mutate(x2 = case_when(class == "A" ~ x2 + 2.5,
                        class == "B" ~ x2 - 2.5,
                        class == "C" ~ x2 + 2.5),
         x1 = case_when(class == "A" ~ x1 - 2.5,
                        class == "B" ~ x1 - 0,
                        class == "C" ~ x1 + 2.5))
```

**2. Prepare two unsupervised datasets by removing the class feature.**

```{r}
sim_df_small_unsupervised <- sim_df_small %>% select(-class)
sim_df_large_unsupervised <- sim_df_large %>% select(-class)
```

**3. For each of these datasets, create a scatterplot. Combine the two plots into a single frame (look up the `patchwork` package to see how to do this!) What is the difference between the two datasets?**

```{r}
sim_df_small %>% 
  ggplot(aes(x = x1, y = x2, color = class)) +
  geom_point() +
  ggtitle("Small differences") -> p1

sim_df_large %>%
  ggplot(aes(x = x1, y = x2, color = class)) +
  geom_point() +
  ggtitle("Large differences") -> p2

p1 + p2
```

The difference between the two datasets is that the small differences dataset has a smaller difference in the values of x1 and x2 for each class compared to the large differences dataset. This can be clearly seen in the side by side scatterplots, where the class separation is more distinct in the large differences dataset, whereas there is much more overlap between the classes in the small differences dataset.

## 2.1 Hierarchical clustering

**4. Run a hierarchical clustering on these datasets and display the result as dendrograms. Use euclidian distances and the complete agglomeration method. Make sure the two plots have the same y-scale. What is the difference between the dendrograms? (Hint: functions youâ€™ll need are `hclust`, `ggdendrogram`, and `ylim`)**

```{r}
# distances <- dist(sim_df_small_unsupervised, method = "euclidean")
# result <- hclust(distances, method = "complete")
# ggdendrogram(result, rotate = FALSE) + ylim(0, 10) + ggtitle("Small differences")
# cutree(result, k = 2)

sim_df_small_unsupervised %>% 
  dist(., method = "euclidean") %>% 
  hclust(method = "complete") %>% 
  ggdendrogram(., rotate = FALSE) + 
  ylim(0, 10) + 
  ggtitle("Small differences, Euclidian") -> d1

sim_df_large_unsupervised %>%
  dist(, method = "euclidean") %>% 
  hclust(method = "complete") %>% 
  ggdendrogram(., rotate = FALSE) + 
  ylim(0, 10) + 
  ggtitle("Large differences, Euclidian") -> d2

d1 + d2
```

The difference between de dendograms is that the small difference dataset has a lower mean between-cluster dissimilarity compared to the large difference dataset. This is because the small difference dataset has less variation in the values of x1 and x2 for each class compared to the large difference dataset. This can be seen in the dendograms, where the small difference dataset has shorter branches and a lower mean between-cluster dissimilarity compared to the large difference dataset.

**5. For the dataset with small differences, also run a complete agglomeration hierarchical cluster with manhattan distance.**

```{r}
sim_df_small_unsupervised %>% 
  dist(., method = "manhattan") %>% 
  hclust(method = "complete") %>% 
  ggdendrogram(., rotate = FALSE) + 
  ylim(0, 10) + 
  ggtitle("Small differences, Manhattan") -> d1_manhat

d1 + d1_manhat
```

We can see that the Manhattan distance generally finds greater between-cluster dissimilarity than when using the Euclidian distance metric.

**6. Use the `cutree()` function to obtain the cluster assignments for three clusters and compare the cluster assignments to the 3-cluster euclidian solution. Do this comparison by creating two scatter plots with cluster assignment mapped to the colour aesthetic. Which difference do you see?**

```{r}
#| label: q6
#| fig-width: 12
#| fig-height: 5

sim_df_small_unsupervised %>% 
  dist(., method = "euclidean") %>% 
  hclust(method = "complete") %>% 
  cutree(k = 3) -> clusters_euclidean

sim_df_small_unsupervised %>% 
  dist(., method = "manhattan") %>% 
  hclust(method = "complete") %>% 
  cutree(k = 3) -> clusters_manhattan

sim_df_small %>%
  mutate(cluster_euclidean = clusters_euclidean,
         cluster_manhattan = clusters_manhattan) %>%
  ggplot(aes(x = x1, y = x2, color = as.factor(cluster_euclidean))) +
  geom_point() +
  ggtitle("Euclidean") -> p1

sim_df_small %>%
  mutate(cluster_euclidean = clusters_euclidean,
         cluster_manhattan = clusters_manhattan) %>%
  ggplot(aes(x = x1, y = x2, color = as.factor(cluster_manhattan))) +
  geom_point() +
  ggtitle("Manhattan") -> p2

p1 + p2
```

The Euclidian solution has slightly different class assignment, with more people in the first (red) class and less in the second (green) class than the Manhattan solution. This shows that the choice of distance metric can have an impact on the clustering results. More generally, it seems that the Euclidian clustering method prefers more circular classes and the Manhattan clustering method more rectangular classes.

# 3 Practical exercises

## 3.1 K-means clustering

**7. Create k-means clusterings with 2, 3, 4, and 6 classes on the large difference data. Again, create coloured scatter plots for these clusterings.**

```{r}
#| fig-width: 12

sim_df_large_unsupervised %>% 
  kmeans(centers = 2) %>% 
  .$cluster -> clusters_2

sim_df_large_unsupervised %>%
  kmeans(centers = 3) %>% 
  .$cluster -> clusters_3

sim_df_large_unsupervised %>%
  kmeans(centers = 4) %>% 
  .$cluster -> clusters_4

sim_df_large_unsupervised %>%
  kmeans(centers = 6) %>% 
  .$cluster -> clusters_6

sim_df_large %>%
  mutate(cluster_2 = clusters_2,
         cluster_3 = clusters_3,
         cluster_4 = clusters_4,
         cluster_6 = clusters_6) %>%
  ggplot(aes(x = x1, y = x2, color = as.factor(cluster_2))) +
  geom_point() +
  ggtitle("2 clusters") -> p1

sim_df_large %>%
  mutate(cluster_2 = clusters_2,
         cluster_3 = clusters_3,
         cluster_4 = clusters_4,
         cluster_6 = clusters_6) %>%
  ggplot(aes(x = x1, y = x2, color = as.factor(cluster_3))) +
  geom_point() +
  ggtitle("3 clusters") -> p2

sim_df_large %>%
  mutate(cluster_2 = clusters_2,
         cluster_3 = clusters_3,
         cluster_4 = clusters_4,
         cluster_6 = clusters_6) %>%
  ggplot(aes(x = x1, y = x2, color = as.factor(cluster_4))) +
  geom_point() +
  ggtitle("4 clusters") -> p3

sim_df_large %>%
  mutate(cluster_2 = clusters_2,
         cluster_3 = clusters_3,
         cluster_4 = clusters_4,
         cluster_6 = clusters_6) %>%
  ggplot(aes(x = x1, y = x2, color = as.factor(cluster_6))) +
  geom_point() +
  ggtitle("6 clusters") -> p4

p1 + p2 + p3 + p4
```

**8. Do the same thing again a few times. Do you see the same results every time? where do you see differences?**

The results are not the same every time. The differences are most pronounced in the 2-cluster solution, where the cluster assignments can vary quite a bit between runs. The 4-cluster and 6-cluster solutions are also a bit unstable. The 3-cluster solution is the most stable, which is unsurprising considering that this is the data generating mechanism.

**9. Find a way online to perform bootstrap stability assessment for the 3 and 6-cluster solutions.**

```{r}
#| label: q9

library(bootcluster)

stability_3 <- bootcluster::stability(sim_df_large_unsupervised, k = 3, B = 100)
stability_6 <- bootcluster::stability(sim_df_large_unsupervised, k = 6, B = 100)

stability_3$overall
stability_6$overall
```

The overall stability of the 3-cluster solution is 98.8%, which is very high, but unsurprising given that this is the data generating mechanism. The overall stability of the 6-cluster solution is 73.2%, which is substantially lower. This indicates that the 6-cluster solution is less stable than the 3-cluster solution. We can take a higher degree of stability as evidence that a given number of clusters may more a more accurate reflection of the underlying data structure.

# 4 Challenge question

**10. Create a function to perform k-medians clustering**

Write this function from scratch: you may use base-R and tidyverse functions. Use Euclidean distance as your distance metric.

Input:

-   dataset (as a data frame)

-   K (number of clusters)

Output:

-   a vector of cluster assignments

Tip: use the unsupervised version of `sim_df_large` with `K = 3` as a tryout-dataset

```{r}
kmedians_clust <- function(data, k){
  
  if (!is.data.frame(data)) {
    stop("Input data must be a data frame.")
  }
  
  # Step 1. Randomly assign examples to k clusters
  n <- nrow(data)
  k_vec <- 1:k
  data$cluster <- sample(k_vec, 
                     size = n, 
                     replace = TRUE)
  
  converged <- FALSE
  
  while (!converged) {
    
    # Step 2. Calculate the centroid (per-feature median) for each cluster
    cluster_median <- data %>% 
      group_by(cluster) %>% 
      summarize_all(median)
    
    # Step 3. Assign each unit the cluster belonging to its closest centroid based on Euclidean distance
    new_clusters <- data %>%
      rowwise() %>%
      mutate(
        # Calculate the Euclidean distance to each cluster centroid and assign the unit to the closest cluster
        cluster = which.min(
          map_dbl(1:k, ~ sum((c_across(where(is.numeric)) - cluster_median[.x, -1])^2))
        )
      ) %>%
      pull(cluster)
    
    # Step 4: Check if the cluster assignment changed
    converged <- all(new_clusters == data$cluster)
    data$cluster <- new_clusters
  }
  
  return(data)
}
```

```{r}
# set the seed for reproducibility
set.seed(123)

# test the function
medians_3_clusters <- kmedians_clust(sim_df_large_unsupervised, 3)

# plot the results
medians_3_clusters %>%
  ggplot(aes(x = x1, y = x2, color = as.factor(cluster))) +
  geom_point() +
  ggtitle("K-medians clustering with 3 clusters")

# test the function on the small differences dataset
medians_3_clusters_small <- kmedians_clust(sim_df_small_unsupervised, 3)

# plot the results
medians_3_clusters_small %>%
  ggplot(aes(x = x1, y = x2, color = as.factor(cluster))) +
  geom_point() +
  ggtitle("K-medians clustering with 3 clusters (small differences)")
```

**11. Add an input parameter `smart_init`. If this is set to `TRUE`, initialize cluster assignments using hierarchical clustering (from `hclust`). Using the unsupervised sim_df_small, look at the number of iterations needed when you use this method vs when you randomly initialize.**

```{r}
kmedians_clust_smart <- function(data, k, smart_init = FALSE){
  
  if (!is.data.frame(data)) {
    stop("Input data must be a data frame.")
  }
  
  # Step 1: Initialize cluster assignments
  if (smart_init) {
    # Initialize cluster assignments using hierarchical clustering
    distances <- dist(data, method = "euclidean")
    result <- hclust(distances, method = "complete")
    data$cluster <- cutree(result, k = k)
  } else {
    # Randomly assign examples to k clusters
    n <- nrow(data)
    k_vec <- 1:k
    data$cluster <- sample(k_vec, 
                           size = n, 
                           replace = TRUE)
  }
  
  # initially counter variable and convergence flag
  iterations <- 0
  converged <- FALSE
  
  while (!converged) {
    
    # Step 2. Calculate the centroid (per-feature median) for each cluster
    cluster_median <- data %>% 
      group_by(cluster) %>% 
      summarize_all(median)
    
    # Step 3. Assign each unit the cluster belonging to its closest centroid based on Euclidean distance
    new_clusters <- data %>%
      rowwise() %>%
      mutate(
        # Calculate the Euclidean distance to each cluster centroid and assign the unit to the closest cluster
        cluster = which.min(
          map_dbl(1:k, ~ sum((c_across(where(is.numeric)) - cluster_median[.x, -1])^2))
        )
      ) %>%
      pull(cluster)
    
    # Step 4: Check if the cluster assignment changed
    converged <- all(new_clusters == data$cluster)
    data$cluster <- new_clusters
    
    # count iterations
    iterations <- iterations + 1
  }
  
   # print some information
  cat("K-medians with K =", k, "| Iterations:", iterations, "| converged:", converged, "\n")
  
  return(data)
}
```

```{r}
# test the function with smart initialization
medians_3_clusters_smart <- kmedians_clust_smart(sim_df_small_unsupervised, 3, smart_init = TRUE)

# test the function without smart initialization
medians_3_clusters_random <- kmedians_clust_smart(sim_df_small_unsupervised, 3, smart_init = FALSE)
```

Without smart initialization, the function converges in 5-6 iterations. With smart initialization, the function converges in 2 iterations. This shows that smart initialization can reduce the number of iterations needed for convergence, thereby speeding up the clustering process.
