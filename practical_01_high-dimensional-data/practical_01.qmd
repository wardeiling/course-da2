---
title: "Practical 1: Dealing with High Dimensional Data"
author: "Ward B. Eiling"
date: "14-11-2024"
format: 
  html:
    self-contained: true
execute:
  warning: false
  message: false
---

```{r}
# Load libraries
library(tidyverse)
library(glmnet)
library(caret)
library(here)
```

# 1 Introduction

In this practical, we will deal with the curse of dimensionality by applying the "bet on sparsity". We will use the following packages in the process:

```         
library(tidyverse)
library(glmnet)
```

Create a practical folder with an .Rproj file (e.g., `practical_01.Rproj`) and a data folder inside. Download the prepared files below and put them in the data folder in your project directory.

-   [gene_expressions.rds](https://infomda2.nl/practicals/01_high_dimensional/data/gene_expressions.rds)

-   [phenotypes.rds](https://infomda2.nl/practicals/01_high_dimensional/data/phenotypes.rds)

# 2 Take Home Exercises

## 2.1 Gene expression data

The data file we will be working with is gene expression data. Using microarrays, the expression of many genes can be measured at the same time. The data file contains expressions for 54675 genes with IDs such as `1007_s_at`, `202896_s_at`, `AFFX-r2-P1-cre-3_at`. (NB: these IDs are specific for this type of chip and need to be converted to actual gene names before they can be looked up in a database such as "GeneCards"). The values in the data file are related to the amount of RNA belonging to each gene found in the tissue sample.

The goal of the study for which this data was collected is one of exploratory cancer classification: are there differences in gene expression between tissue samples of human prostates with and without prostate cancer?

**1. Read the data file `gene_expressions.rds` using `read_rds()`. What are the dimensions of the data? What is the sample size?**

```{r}
gene_exp <- read_rds(here("practical_01_high-dimensional-data/data/gene_expressions.rds"))
gene_exp <- as.data.frame(gene_exp)

dim(gene_exp)[1] # 237 rows/cases
dim(gene_exp)[2] # 54676 columns/features
# Thus, P > N

length(unique(gene_exp$sample))
```

**2. As always, visualisation is a good idea. Create histograms of the first 6 variables. Describe what you notice.**

```{r}
firstsix <- gene_exp[,2:7]
coln <- colnames(firstsix)

par(mfrow = c(2,3))
for (i in 1:6) {
  hist(firstsix[,i], main = coln[i], xlab = "Values", col = "skyblue")
}
```

We can observe that some variables have an approximate normal distributed, albeit with a slight skew (1007_s_at/1053_at/121_at/1294_at), whereas others (1255_g_at/117_at) clearly have a non-symmetric distributed dominated by lower values, indicating lower expression of those genes.

**3. Load phenotypes, select relevant columns, and join with the gene expression data.**

```{r}
phenotypes <- read_rds(here("practical_01_high-dimensional-data/data/phenotypes.rds")) %>%
  select("sample", "disease")

gene_exp_dis <- right_join(gene_exp, phenotypes, by = "sample")
```

**4. Does this dataset suffer from class imbalance?**

```{r}
table(gene_exp_dis$disease)

# Observations
# The dataset is balanced for normal/tumor samples, but distributions for variables by disease status may differ.
```

No, the classes are relatively balanced (about 50/50) with 116 healthy samples and 121 tumor samples.

**5. Split the data into a training (80%) and a test set (20%).**

```{r}
# recode disease into dummy variable where normal = 0 and tumor = 1
gene_exp_dis$disease_dum <- NA
gene_exp_dis$disease_dum[gene_exp_dis$disease == "normal"] <- 1
gene_exp_dis$disease_dum[gene_exp_dis$disease == "tumor"] <- 2
gene_exp_dis$disease_dum <- as.numeric(gene_exp_dis$disease_dum)

set.seed(123)
train_index <- sample(1:nrow(gene_exp_dis), 0.8*nrow(gene_exp_dis))
train <- gene_exp_dis[train_index,]
test <- gene_exp_dis[-train_index,]
```

# 3 Lab Exercises

## 3.1 Correlation filter & logistic regression

**6. Use a correlation filter to find the IDs of the 10 genes most related to disease status.**

```{r}
genes_train <- train %>% select(-sample, -disease, -disease_dum)
cor_fil <- cor(genes_train, train$disease_dum)
cor_abs <- abs(cor_fil)

# Retrieve top correlations
cor_greatest <- cor_abs %>% as.data.frame() %>% arrange(desc(cor_abs)) %>% head(10)
(sel_genes <- rownames(cor_greatest))
```

**7. Perform logistic regression, predicting the outcome using the selected genes.**

```{r}
train_sel_gens <- train %>% select(sel_genes, disease)
train_sel_gens$disease <- as.factor(train_sel_gens$disease)
fit_lr <- glm(disease ~ ., data = train_sel_gens, family = "binomial")
summary(fit_lr)
```

From the logistic regression we can see that none of the genes have a p-value lower than 0.05. In fact, only two genes have a p-value below 0.1. This makes sense as we only used 80% of an already small dataset, making the SE substantial.

**8. Create a confusion matrix for the predictions of this model on the test set.**

```{r}
test_sel_gens <- test %>% select(sel_genes, disease, sample)
predictions_prob_test <- predict(fit_lr, newdata = test_sel_gens, type = "response")

predictions_df <- data.frame(
  sample = test_sel_gens$sample, 
  observed = test_sel_gens$disease, 
  test_pred_prob = predictions_prob_test
)

predictions_df$test_pred <- ifelse(predictions_df$test_pred_prob > 0.5, "tumor", "normal")
predictions_df$test_pred <- as.factor(predictions_df$test_pred)
predictions_df$observed <- as.factor(predictions_df$observed)

(cor_log_reg_confusion <- confusionMatrix(predictions_df$test_pred, predictions_df$observed))
```

Of the normal samples, our model predicted 24 correct, mistakenly classifying 3 samples as tumor (i.e., false positive). Of the tumor samples, our model predicted 20 correct, mistakenly classifying 1 sample as a normal sample (i.e., a false negative). In the context of the current research question, we would probably care most about minimizing the false negative rate, as we would not like to send patients home who do have a tumor.

## 3.2 Regularized regression

**9. Prepare your data for input into `glmnet`.**

```{r}
x_train <- as.matrix(train %>% select(-sample, -disease_dum, -disease))
y_train <- as.matrix(train %>% select(disease) %>% mutate(disease = as.factor(disease)))
x_test <- as.matrix(test %>% select(-sample, -disease_dum, -disease))
y_test <- as.matrix(test %>% select(disease) %>% mutate(disease = as.factor(disease)))
```

**10. Use the glmnet function to fit a LASSO regression.**

```{r}
lasso <- glmnet(x_train, y_train, family = "binomial")
plot(lasso)
```

In this plot we can see each line representing a coefficient in the model, with its value on the Y-axis. On the X-axis we can see the total budget. We can see how an an increase in the L1 norm results in a lower penalty/constraint and thus allows a greater budget for the coefficients. As this budget decreases near the left, more parameters shrink to zero.

**11. Use `cv.glmnet` for cross-validated LASSO regression.**

```{r}
cv_lasso <- cv.glmnet(x_train, y_train, family = "binomial")
plot(cv_lasso)
```

In this figure we can see the deviance (lack of fit) on the Y-axis and the log of the penalty parameter lambda on the X-axis. The number of non-zero parameters is given on top. Thus, we can see that with an increase in the log of the penalty, more parameters are set to zero, which in turn, increases the bias in the model fit. If we care only about predictive performance, we should select the setting with 56 non-zero parameters as the deviance is the lowest here. Alternatively, if we mainly care about interpretability, we may think about how many non-zero parameters we think would be plausible based on the data-generating mechanism.

**12. Inspect the nonzero coefficients of the model with the lowest out-of-sample deviance.**

```{r}
# (cv_coef_SE <- as.data.frame(as.matrix(coef(cv_lasso, s = "lambda.1se"))) %>%
#   filter(s1 != 0))
cv_coef_min <- as.data.frame(as.matrix(coef(cv_lasso, s = "lambda.min"))) %>%
  filter(s1 != 0)
head(cv_coef_min)
```

```{r}
intersect(sel_genes, rownames(cv_coef_min))
```

There are 5 overlapping genes between the two models, so there is partial overlap.

**13. Use the `predict()` function on the fitted `cv.glmnet` object to predict disease status for the test set.**

```{r}
predicted_cv_lasso <- predict(
  object = cv_lasso,
  newx = x_test,
  s = "lambda.min",
  type = "response"
)

predicted <- ifelse(predicted_cv_lasso[,"lambda.min"] > 0.5, "tumor", "normal")
lasso_cv_confusion <- confusionMatrix(as.factor(predicted), reference = as.factor(y_test))

cor_log_reg_confusion$table # Logistic regression accuracy: 91.67%
lasso_cv_confusion$table    # LASSO with CV accuracy: 95.83%
```

Both methods performed well, but LASSO performed slightly better with a greater predictive accuracy on the test set. This is unsurprising as this method performs variable selection in a more rigorous manner.
